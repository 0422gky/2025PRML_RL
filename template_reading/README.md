# è¿™é‡Œå­˜æ”¾äº†ä¸€äº›å†™ä»£ç æ—¶éœ€è¦é˜…è¯»çš„template

## é˜…è¯»reward model, value model(answer from copliot)
è¦çœ‹ reward/value æ¨¡å‹çš„æºä»£ç ï¼Œæœ‰ä¸¤ç§æ¥æºï¼šHugging Face Hub ä¸Šçš„æ¨¡å‹ä»“åº“ï¼ˆè‹¥ trust_remote_code ä¸º True ä¼šç”¨ä»“åº“è‡ªå¸¦çš„ Python æ¨¡å—ï¼‰ï¼Œæˆ– Transformers å†…ç½®æ¨¡å‹ç±»ï¼ˆé»˜è®¤ä» config.architectures æ¨æ–­ï¼Œå¦‚ GPTNeoXForSequenceClassificationï¼‰ã€‚

æ­¥éª¤å»ºè®®ï¼š
- å…‹éš†æ¨¡å‹ä»“åº“ï¼ŒæŸ¥çœ‹æ˜¯å¦åŒ…å«è‡ªå®šä¹‰ä»£ç 
````bash
git lfs install
git clone https://huggingface.co/cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr /workspace/models/reward
ls -la /workspace/models/reward
# å…³æ³¨ï¼šmodeling_*.pyã€modeling.pyã€custom codeã€READMEã€config.json
````

- ç¡®è®¤å®é™…åŠ è½½åˆ°çš„ç±»ï¼ˆä»¥åŠæ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰ä»£ç ï¼‰
````python
from transformers import AutoConfig, AutoModelForSequenceClassification
name = "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr"
cfg = AutoConfig.from_pretrained(name)
print("architectures:", cfg.architectures, "model_type:", cfg.model_type)

m = AutoModelForSequenceClassification.from_pretrained(name, trust_remote_code=False)
print("loaded class:", type(m), "module:", m.__module__)
````

- è‹¥æ²¡æœ‰è‡ªå®šä¹‰ä»£ç ï¼Œåˆ™åˆ° Transformers æŸ¥çœ‹å¯¹åº”å†…ç½®ç±»æºç ï¼ˆä¾‹å¦‚ GPTNeoXForSequenceClassificationï¼‰
````bash
python -c "import importlib; m=importlib.import_module('transformers.models.gpt_neox.modeling_gpt_neox'); print(m.__file__)"
# æ‰“å¼€ä¸Šè¿°æ–‡ä»¶è·¯å¾„ï¼Œåœ¨ VS Code é‡Œé˜…è¯» GPTNeoXForSequenceClassification çš„å®ç°
````

- åœ¨çº¿æºç å‚è€ƒï¼ˆTransformersï¼‰
  - GPTNeoXForSequenceClassification: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py

è¯´æ˜ï¼š
- æœ¬è„šæœ¬ä¸­ value_model ä¸ reward_model éƒ½ä»åŒä¸€ä¸ª checkpoint åŠ è½½ï¼š
  - AutoModelForSequenceClassification(..., num_labels=1)
- æ˜¯å¦ä½¿ç”¨ä»“åº“è‡ªå®šä¹‰ä»£ç å–å†³äº trust_remote_codeï¼ˆç”± model_args.trust_remote_code å†³å®šï¼‰ã€‚å¦‚ä»“åº“å†…å« modeling*.py ä¸”ä½ è®¾ä¸º Trueï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ä»“åº“å®ç°ï¼›å¦åˆ™ä½¿ç”¨ Transformers å†…ç½®å®ç°ã€‚


# reward model å…·ä½“æ¶æ„(answer from GPT)
è¿™æ˜¯ä¸€ä¸ª**éå¸¸å…³é”®ã€è€Œä¸”ä½ å·²ç»é—®åˆ°â€œçœŸæ­£è¯¥çœ‹çš„åœ°æ–¹â€**çš„é—®é¢˜ ğŸ‘
ç»“è®ºå…ˆç»™ä½ ï¼š

> **`cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr` æœ¬èº«ä¸åŒ…å«â€œç®—æ³•æºç â€ï¼Œ
> å®ƒåªæ˜¯ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„ reward model checkpointã€‚
> çœŸæ­£çš„â€œå®ç°ä»£ç â€åˆ†æˆä¸¤éƒ¨åˆ†ï¼š
> 1ï¸âƒ£ æ¨¡å‹ forward ç»“æ„ â†’ åœ¨ ğŸ¤— Transformers é‡Œ
> 2ï¸âƒ£ è¿™ä¸ª reward model æ˜¯â€œæ€ä¹ˆè®­ç»ƒå‡ºæ¥çš„â€ â†’ åœ¨ CleanRL / OpenAI TL;DR ç›¸å…³ä»£ç é‡Œ**

## äºŒã€ä½ çœŸæ­£è¯¥çœ‹â€œreward model å¦‚ä½•è®¡ç®—åˆ†æ•°â€çš„ 3 ä¸ªå±‚çº§

### ğŸ”¹ å±‚çº§ 1ï¼ˆæœ€é‡è¦ï¼‰ï¼šæ¨¡å‹ forward åœ¨å“ªé‡Œå®ç°ï¼Ÿ

Reward model åœ¨ TRL é‡Œæ˜¯è¿™æ ·åŠ è½½çš„ï¼š

```python
AutoModelForSequenceClassification.from_pretrained(
    reward_model_path,
    num_labels=1,
)
```

è¿™å¥è¯çš„æ„æ€æ˜¯ï¼š

> **â€œæˆ‘ç”¨ä¸€ä¸ªæ ‡å‡†çš„ Transformer backbone + ä¸€ä¸ª sequence classification headâ€**

### é‚£æºç åœ¨å“ªé‡Œï¼Ÿ

**åœ¨ Transformersï¼Œè€Œä¸æ˜¯ cleanrlã€‚**

å› ä¸º `cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr` çš„ backbone æ˜¯ **Pythiaï¼ˆGPT-NeoXï¼‰**ã€‚

æ‰€ä»¥ä½ åº”è¯¥çœ‹çš„æ˜¯ï¼š

```
transformers/models/gpt_neox/modeling_gpt_neox.py
```

é‡ç‚¹ç±»æ˜¯ï¼š

```python
class GPTNeoXForSequenceClassification(...)
```

#### æ€ä¹ˆæœ€å¿«å®šä½ï¼Ÿ

åœ¨ä½ çš„æœåŠ¡å™¨ä¸Šç›´æ¥è·‘ï¼š

```bash
python - <<'PY'
from transformers import AutoModelForSequenceClassification
import inspect, os

model = AutoModelForSequenceClassification.from_pretrained(
    "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr",
    num_labels=1,
)

print(model.__class__)
print(os.path.abspath(inspect.getfile(model.__class__)))
PY
```

ä½ ä¼šçœ‹åˆ°ç±»ä¼¼ï¼š

```text
<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForSequenceClassification'>
/root/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py
```

ğŸ‘‰ **è¿™å°±æ˜¯ reward model forward çš„â€œçœŸå®æºç ä½ç½®â€**

ä½ è€å¸ˆè®©ä½ çœ‹çš„ï¼š

* reward æ˜¯æ€ä¹ˆä» hidden states ç®—å‡ºæ¥çš„
* ç”¨çš„æ˜¯å“ªä¸€ä¸ª tokenï¼ˆé€šå¸¸æ˜¯ last non-pad tokenï¼‰
* head æ˜¯ä¸æ˜¯ `Linear(hidden_size â†’ 1)`

**å…¨éƒ¨éƒ½åœ¨è¿™ä¸ªæ–‡ä»¶é‡Œã€‚**

---

## ä¸‰ã€è¿™ä¸ª reward model â€œå½“å¹´æ˜¯æ€ä¹ˆè®­ç»ƒå‡ºæ¥çš„â€ï¼Ÿ

è¿™æ˜¯ä½ ç°åœ¨é—®çš„**ç¬¬äºŒå±‚å«ä¹‰**ï¼Œä¹Ÿæ˜¯å¾ˆå¤šäººä¼šå¿½ç•¥çš„ã€‚

### `cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr` æ¥æº

å®ƒæ¥è‡ª **OpenAI TL;DR / CleanRL ä½“ç³»**ï¼Œæœ¬è´¨æ˜¯ï¼š

> ç”¨ **äººç±»åå¥½æ•°æ®ï¼ˆchosen vs rejected TL;DRï¼‰**
> è®­ç»ƒä¸€ä¸ª pairwise preference reward model

è®­ç»ƒç›®æ ‡é€šå¸¸æ˜¯ï¼š

[
\mathcal{L}
===========

-\log \sigma(r_{\text{chosen}} - r_{\text{rejected}})
]

---

### è¿™äº›â€œè®­ç»ƒ reward model çš„ä»£ç â€åœ¨å“ªé‡Œï¼Ÿ

ä¸åœ¨ TRLï¼Œä¹Ÿä¸åœ¨è¿™ä¸ª HF repoï¼Œè€Œåœ¨ **CleanRL / OpenAI çš„å‚è€ƒå®ç°**ã€‚

ä½ å¯ä»¥çœ‹ä¸¤ä¸ªåœ°æ–¹ï¼ˆä»»é€‰å…¶ä¸€å³å¯å†™æŠ¥å‘Šï¼‰ï¼š

#### 1ï¸âƒ£ OpenAI TL;DR åŸå§‹è®ºæ–‡ + é™„å½•

> *Learning to Summarize from Human Feedback* (Stiennon et al., 2020)

è¿™ç¯‡è®ºæ–‡**å®šä¹‰äº† reward model çš„è®­ç»ƒæ–¹å¼**ï¼Œè¿™æ˜¯æœ€æƒå¨æ¥æºã€‚

---

#### 2ï¸âƒ£ CleanRL çš„ reward model è®­ç»ƒè„šæœ¬ï¼ˆå·¥ç¨‹å®ç°ï¼‰

CleanRL é¡¹ç›®é‡Œæœ‰å¯¹åº”çš„ reward model è®­ç»ƒä»£ç ï¼Œä½ å¯ä»¥åœ¨ GitHub æœï¼š

```text
cleanrl reward model tldr
```

å…¸å‹ç»“æ„æ˜¯ï¼š

* ä¸€ä¸ª `AutoModelForSequenceClassification`
* è¾“å…¥ `prompt + completion`
* ç”¨ preference loss è®­ç»ƒ

ğŸ‘‰ **ä½ ä¸éœ€è¦é€è¡Œå¤ç°å®ƒ**
è€å¸ˆåªè¦æ±‚ä½ â€œç†è§£ reward model æ˜¯æ€ä¹ˆæ¥çš„ã€æ€ä¹ˆç”¨çš„â€ã€‚

---

## å››ã€å›åˆ°ä½ ç°åœ¨ PPO ä»£ç ä¸­ï¼šreward æ˜¯â€œæ€ä¹ˆè¢«ç”¨â€çš„ï¼Ÿ

ä½ è€å¸ˆåœ¨ä½œä¸šé‡Œæ˜ç¡®è¯´äº†è¦åˆ†æï¼š

> * å¥–åŠ±è®¡ç®—æ–¹å¼
> * å¥–åŠ±å½’ä¸€åŒ–
> * å¥–åŠ±ä¸ç­–ç•¥æ›´æ–°çš„äº¤äº’é€»è¾‘

è¿™ä¸‰ç‚¹**ä¸åœ¨ cleanrl repoï¼Œè€Œåœ¨ TRL çš„ PPOTrainer**ã€‚

### æ‰€ä»¥ä½ ç°åœ¨é˜…è¯»çš„æ­£ç¡®é¡ºåºæ˜¯ï¼š

1ï¸âƒ£ **Transformers**

* `GPTNeoXForSequenceClassification.forward`
* çœ‹ `logits` æ€ä¹ˆå˜æˆä¸€ä¸ª scalar reward

2ï¸âƒ£ **TRL**

* `PPOTrainer` é‡Œï¼š

  * å“ªé‡Œè°ƒç”¨ `reward_model(**inputs)`
  * reward æ˜¯å¦åšäº† normalize / clip
  * reward å¦‚ä½•è¿›å…¥ advantage / PPO loss

3ï¸âƒ£ **è®ºæ–‡ / CleanRL**

* reward model çš„è®­ç»ƒç›®æ ‡ï¼ˆç†è®ºæ¥æºï¼‰

---

## äº”ã€ä½ å¯ä»¥ç›´æ¥å†™è¿›æŠ¥å‘Šçš„ä¸€å¥â€œå®šä½è¯´æ˜â€ï¼ˆéå¸¸åŠ åˆ†ï¼‰

> *The reward model is implemented as a sequence classification model based on a GPT-NeoX backbone, which outputs a scalar score for each promptâ€“response pair. The model architecture and forward computation follow the standard implementation in the Hugging Face Transformers library, while the model weights are pretrained on TL;DR human preference data.*

è¿™å¥è¯**å®Œç¾å›ç­”â€œreward model çš„å®ç°åœ¨å“ªé‡Œâ€**ã€‚

---

## å…­ã€å¦‚æœä½ æ„¿æ„ï¼Œä¸‹ä¸€æ­¥æˆ‘å¯ä»¥ç›´æ¥å¸®ä½ åšä¸€ä»¶äº‹

ğŸ‘‰ **æ‹¿ `GPTNeoXForSequenceClassification.forward()`ï¼Œé€è¡Œå¸®ä½ è§£é‡Šï¼š
reward æ˜¯æ€ä¹ˆä» token hidden states å˜æˆä¸€ä¸ª scalar çš„**

è¿™æ˜¯è€å¸ˆæœ€å–œæ¬¢çœ‹åˆ°ä½ â€œçœŸçš„è¯»è¿‡æºç â€çš„åœ°æ–¹ã€‚


# è°ƒç”¨çš„reward modelæ¶æ„
```
{
  "_name_or_path": "EleutherAI/pythia-1b-deduped",
  "architectures": [
    "GPTNeoXForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 2048,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 8,
  "num_hidden_layers": 16,
  "rope_scaling": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.33.2",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}
```

```
model = AutoModelForSequenceClassification.from_pretrained(
    "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr",
    num_labels=1,
    use_safetensors=True,     
)


python - <<'PY'
from transformers import AutoModelForSequenceClassification
import inspect, os

model = AutoModelForSequenceClassification.from_pretrained(
    "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr",
    num_labels=1,
    use_safetensors=True,     
)

print(model.__class__)
print(os.path.abspath(inspect.getfile(model.__class__)))
PY

```

# reward model å…·ä½“å®ç°(ä¸»è¦çœ‹GPTSequenceClassificationæ€ä¹ˆæŠŠtokenå˜æˆlogit scoreè¿˜æœ‰å…³äºæœ€åä¸€ä½çš„pad-token-idçš„å¯»æ‰¾)
å¯¹äºä¸€ä¸ªå¥å­ï¼ˆprompt+responseï¼‰å¯èƒ½ä¼šæœ‰å¾ˆå¤šä¸ªtokenç»„æˆï¼Œæˆ‘ä»¬åšçš„å°±æ˜¯å¯¹äºæ¯”å¦‚è¯´ä¸€å¥å¥å­å½“ä¸­100ä¸ªtokenï¼Œåªå¯¹äºæœ€åä¸€ä¸ªç®—scoreï¼Œç„¶åå…¶ä»–çš„tokenç»™ä¸€ä¸ªKL loss ä½œä¸ºrewardå€¼ï¼Œç”¨äºçº¦æŸrefå’Œç°åœ¨æ¨¡å‹ä¹‹é—´çš„å·®åˆ«
è·¯å¾„ï¼š`/root/miniconda3/envs/trl/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py`
```python

@auto_docstring(
    custom_intro="""
    The GPTNeoX Model transformer with a sequence classification head on top (linear layer).

    [`GPTNeoXForSequenceClassification`] uses the last token in order to do the classification, as other causal models
    (e.g. GPT-1) do.

    Since it does classification on the last token, it requires to know the position of the last token. If a
    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
    each row of the batch).
    """
)
# ç”¨last tokenåšclassification, è¦çŸ¥é“æœ€åä¸€ä¸ªtokençš„ä½ç½®ï¼Œå¦‚æœconfigurationä¸­æœ‰'pad_token_id' åˆ¤æ–­æ˜¯ä¸æ˜¯padding token(find the last token in each row of the batch)
class GPTNeoXForSequenceClassification(GPTNeoXPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.gpt_neox = GPTNeoXModel(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False) # score çš„è®¡ç®—ï¼šæœ€åé€šè¿‡ä¸€ä¸ªLinearå±‚ï¼Œ

        # Initialize weights and apply final processing
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        past_key_values: Optional[Union[Cache, tuple[tuple[torch.FloatTensor]]]] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> SequenceClassifierOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        # config.num_labelsçš„ä¸ªæ•°å†³å®šäº†æœ€ç»ˆçš„lossè®¡ç®—æ–¹å¼
        outputs: BaseModelOutputWithPast = self.gpt_neox(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        hidden_states = outputs.last_hidden_state
        logits = self.score(hidden_states)

        batch_size = logits.shape[0]
        # å¦‚æœæ¨¡å‹é…ç½®é‡Œæ ¹æœ¬æ²¡æœ‰ pad_token_idï¼Œé‚£å®ƒæ— æ³•å¯é åŒºåˆ† paddingï¼Œå°±ä¸å…è®¸ batch>1ï¼ˆå› ä¸ºä¸åŒæ ·æœ¬é•¿åº¦ä¸ä¸€æ ·ä¼šä¹±ï¼‰ã€‚
        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
        if self.config.pad_token_id is None:
            last_non_pad_token = -1
        elif input_ids is not None:
            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id
            # æ‰¾æœ€å³è¾¹çš„non pad token id
            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)
            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)
            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)
        else:
            last_non_pad_token = -1
            logger.warning_once(
                f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
                "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
            )

        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )



class GPTNeoXForTokenClassification(GPTNeoXPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.gpt_neox = GPTNeoXModel(config)
        self.dropout = nn.Dropout(config.classifier_dropout)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        # Initialize weights and apply final processing
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> TokenClassifierOutput:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        outputs: BaseModelOutputWithPast = self.gpt_neox(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        hidden_states = outputs.last_hidden_state
        hidden_states = self.dropout(hidden_states)
        logits = self.classifier(hidden_states)

        loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.config)

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


@auto_docstring
class GPTNeoXForQuestionAnswering(GPTNeoXPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.gpt_neox = GPTNeoXModel(config)
        self.qa_outputs = nn.Linear(config.hidden_size, 2)

        # Initialize weights and apply final processing
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        start_positions: Optional[torch.LongTensor] = None,
        end_positions: Optional[torch.LongTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> QuestionAnsweringModelOutput:
        outputs: BaseModelOutputWithPast = self.gpt_neox(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        sequence_output = outputs.last_hidden_state

        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1).contiguous()
        end_logits = end_logits.squeeze(-1).contiguous()

        loss = None
        if start_positions is not None and end_positions is not None:
            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions)

        return QuestionAnsweringModelOutput(
            loss=loss,
            start_logits=start_logits,
            end_logits=end_logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


__all__ = [
    "GPTNeoXForCausalLM",
    "GPTNeoXForQuestionAnswering",
    "GPTNeoXForSequenceClassification",
    "GPTNeoXForTokenClassification",
    "GPTNeoXLayer",
    "GPTNeoXModel",
    "GPTNeoXPreTrainedModel",
]
```

# PPO trainer
è®­ç»ƒPPOçš„ä»£ç 
è·¯å¾„ï¼š`/root/miniconda3/envs/trl/lib/python3.10/site-packages/trl/experimental/ppo/ppo_trainer.py`
reward model pathå°±æ˜¯ä¹‹å‰çš„cleanrl/tldr
```bash
Args
args : [experimental.ppo.PPOConfig]
Training arguments.

processing_class : [~transformers.PreTrainedTokenizerBase], [~transformers.BaseImageProcessor], [~transformers.FeatureExtractionMixin] or [~transformers.ProcessorMixin]
Class to process the data.

model : torch.nn.Module
Model to be trained. This is the policy model.

ref_model : torch.nn.Module, optional
Reference model used to compute the KL divergence. If None, a copy of the policy model is created.

reward_model : torch.nn.Module
Reward model used to compute the rewards.

train_dataset : [~datasets.Dataset]
Dataset for training.

value_model : torch.nn.Module
Value model used to predict the value of a state.

data_collator : [~transformers.DataCollatorWithPadding], optional
Data collator to batch and pad samples from the dataset. If None, a default data collator is created using the processing_class.

eval_dataset : [~datasets.Dataset] or dict of [~datasets.Dataset], optional
Dataset for evaluation.

optimizers : tuple of torch.optim.Optimizer and torch.optim.lr_scheduler.LambdaLR, optional, defaults to (None, None)
Tuple containing the optimizer and the learning rate scheduler to use for training. If None, the optimizer and the learning rate scheduler are created using the [~transformers.Trainer.create_optimizer_and_scheduler] method.

callbacks : list of [~transformers.TrainerCallback], optional
Callbacks to use during training.

peft_config : [~peft.PeftConfig], optional
PEFT configuration to use PEFT for training. If None, PEFT is not used. If provided, the policy model will be wrapped with the specified PEFT adapter.
```

```python
class PPOTrainer(BaseTrainer):
    """Trainer for Proximal Policy Optimization (PPO).

    For details on PPO, see the paper: [Proximal Policy Optimization
    Algorithms](https://huggingface.co/papers/1707.06347).

    Args:
        args ([`experimental.ppo.PPOConfig`]):
            Training arguments.
        processing_class ([`~transformers.PreTrainedTokenizerBase`], [`~transformers.BaseImageProcessor`], [`~transformers.FeatureExtractionMixin`] or [`~transformers.ProcessorMixin`]):
            Class to process the data.
        model (`torch.nn.Module`):
            Model to be trained. This is the policy model.
        ref_model (`torch.nn.Module`, *optional*):
            Reference model used to compute the KL divergence. If `None`, a copy of the policy model is created.
        reward_model (`torch.nn.Module`):
            Reward model used to compute the rewards.
        train_dataset ([`~datasets.Dataset`]):
            Dataset for training.
        value_model (`torch.nn.Module`):
            Value model used to predict the value of a state.
        data_collator ([`~transformers.DataCollatorWithPadding`], *optional*):
            Data collator to batch and pad samples from the dataset. If `None`, a default data collator is created
            using the `processing_class`.
        eval_dataset ([`~datasets.Dataset`] or `dict` of [`~datasets.Dataset`], *optional*):
            Dataset for evaluation.
        optimizers (`tuple` of `torch.optim.Optimizer` and `torch.optim.lr_scheduler.LambdaLR`, *optional*, defaults to `(None, None)`):
            Tuple containing the optimizer and the learning rate scheduler to use for training. If `None`, the
            optimizer and the learning rate scheduler are created using the
            [`~transformers.Trainer.create_optimizer_and_scheduler`] method.
        callbacks (`list` of [`~transformers.TrainerCallback`], *optional*):
            Callbacks to use during training.
        peft_config ([`~peft.PeftConfig`], *optional*):
            PEFT configuration to use PEFT for training. If `None`, PEFT is not used. If provided, the policy `model`
            will be wrapped with the specified PEFT adapter.
    """

    _tag_names = ["trl", "ppo"]
    _name = "PPO"
    _paper = {
        "title": "Fine-Tuning Language Models from Human Preferences",
        "id": "1909.08593",
        # docstyle-ignore
        "citation": textwrap.dedent("""\
            @article{mziegler2019fine-tuning,
                title        = {{Fine-Tuning Language Models from Human Preferences}},
                author       = {Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul F. Christiano and Geoffrey Irving},
                year         = 2019,
                eprint       = {arXiv:1909.08593}
            }"""),
    }

    def __init__(
        self,
        args: PPOConfig,
        processing_class: PreTrainedTokenizerBase | BaseImageProcessor | FeatureExtractionMixin | ProcessorMixin,
        model: nn.Module,
        ref_model: nn.Module | None,
        reward_model: nn.Module,
        train_dataset: Dataset,
        value_model: nn.Module,
        data_collator: DataCollatorWithPadding | None = None,
        eval_dataset: Dataset | dict[str, Dataset] | None = None,
        # less commonly used
        optimizers: tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
        callbacks: list[TrainerCallback] | None = None,
        peft_config: "PeftConfig | None" = None,
    ) -> None:
        if ref_model is model:
            raise ValueError(
                "`model` and `ref_model` cannot be the same object. If you want `ref_model` to be the "
                "same as `model`, you must make a copy of it, or `None` if you use peft."
            )

        self.args = args
        self.processing_class = processing_class
        self.policy_model = model

        # Define the collator if not provided
        if data_collator is None:
            data_collator = DataCollatorWithPadding(self.processing_class)

        # Handle stop token settings: update policy model's generation_config to use provided stop token
        if args.stop_token and args.stop_token_id:
            raise ValueError("You cannot set both `stop_token` and `stop_token_id`.")
        elif args.stop_token:
            if args.stop_token == "eos":
                self.policy_model.generation_config.eos_token_id = self.stop_token_id = processing_class.eos_token_id
            else:
                raise ValueError(
                    f"Unknown `stop_token` {args.stop_token}. Allowed values are: `'eos'` and `None` (no stop token)."
                )
        else:
            self.policy_model.generation_config.eos_token_id = self.stop_token_id = args.stop_token_id  # None or int

        # Check that the kl estimator is valid
        if self.args.kl_estimator not in {"k1", "k3"}:
            raise ValueError(
                "kl_estimator must be either 'k1' (straightforward, unbiased) or 'k3' (lower variance, unbiased, "
                "appears to be a strictly better estimator). See "
                "[Approximating KL Divergence](http://joschu.net/blog/kl-approx.html) for details."
            )

        # peft support
        if not is_peft_available() and peft_config is not None:
            raise ImportError(
                "PEFT is not installed and you passed a `peft_config` in the trainer's kwargs, please install it to use the PEFT models"
            )
        elif is_peft_available() and peft_config is not None:
            # if model is a peft model and we have a peft_confg, we merge and unload it first
            if isinstance(self.policy_model, PeftModel):
                self.policy_model = self.policy_model.merge_and_unload()

            # get peft model with the given config
            self.policy_model = get_peft_model(self.policy_model, peft_config)
            if args.bf16 and getattr(self.policy_model, "is_loaded_in_4bit", False):
                peft_module_casting_to_bf16(self.policy_model)

        self.is_peft_model = is_peft_available() and isinstance(self.policy_model, PeftModel)
        self.model_adapter_name = args.model_adapter_name
        self.ref_adapter_name = args.ref_adapter_name

        if ref_model:
            self.ref_model = ref_model
        elif self.is_peft_model:
            self.ref_model = None
        else:
            self.ref_model = create_reference_model(self.policy_model)

        self.reward_model = reward_model
        self.train_dataset = train_dataset
        self.train_dataset_len = len(train_dataset)
        self.value_model = value_model
        self.data_collator = data_collator
        self.eval_dataset = eval_dataset
        self.optimizer, self.lr_scheduler = optimizers
        self.optimizer_cls_and_kwargs = None  # needed for transformers >= 4.47

        #########
        # calculate various batch sizes
        #########
        if args.total_episodes is None:  # allow the users to define episodes in terms of epochs.
            args.total_episodes = int(args.num_train_epochs * self.train_dataset_len)
        accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)
        self.accelerator = accelerator
        args.world_size = accelerator.num_processes
        args.local_batch_size = args.per_device_train_batch_size * args.gradient_accumulation_steps
        args.micro_batch_size = int(args.per_device_train_batch_size * args.world_size)
        args.batch_size = int(args.local_batch_size * args.world_size)
        args.mini_batch_size = exact_div(
            args.batch_size, args.num_mini_batches, "`batch_size` must be a multiple of `num_mini_batches`"
        )
        args.local_mini_batch_size = exact_div(
            args.local_batch_size, args.num_mini_batches, "`local_batch_size` must be a multiple of `num_mini_batches`"
        )
        if args.whiten_rewards:
            assert args.local_mini_batch_size >= 8, (
                f"Per-rank minibatch size {args.local_mini_batch_size} is insufficient for whitening"
            )
        # `per_rank_rollout_batch_size` is our `args.local_batch_size`
        # `per_rank_minibatch_size` is our `args.local_mini_batch_size`
        args.num_total_batches = math.ceil(
            args.total_episodes / args.batch_size
        )  # we may train for more than `total_episodes`
        time_tensor = torch.tensor(int(time.time()), device=accelerator.device)
        time_int = broadcast(time_tensor, 0).item()  # avoid different timestamps across processes
        args.run_name = f"{args.exp_name}__{args.seed}__{time_int}"
        self.local_seed = args.seed + accelerator.process_index * 100003  # Prime
        if args.num_sample_generations > 0:
            self.sample_generations_freq = max(1, args.num_total_batches // args.num_sample_generations)
        self.local_dataloader_batch_size = args.local_batch_size

        #########
        # setup model, optimizer, and others
        #########
        for module in [self.policy_model, self.ref_model, self.value_model, self.reward_model]:
            if module is not None:
                disable_dropout_in_model(module)
        self.model = PolicyAndValueWrapper(self.policy_model, self.value_model)
        self.model.config = self.policy_model.config  # needed for pushing to hub
        self.create_optimizer_and_scheduler(
            num_training_steps=args.num_total_batches
        )  # note that we are calling `self.lr_scheduler.step()` manually only at the batch level

        #########
        # trainer specifics
        #########
        default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)
        self.callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks
        self.callback_handler = CallbackHandler(
            self.callbacks, self.model, self.processing_class, self.optimizer, self.lr_scheduler
        )
        self.add_callback(PrinterCallback if self.args.disable_tqdm else DEFAULT_PROGRESS_CALLBACK)
        self.control = TrainerControl()
        self.state = OnlineTrainerState(
            is_local_process_zero=self.is_local_process_zero(),
            is_world_process_zero=self.is_world_process_zero(),
            stateful_callbacks=[
                cb for cb in self.callback_handler.callbacks + [self.control] if isinstance(cb, ExportableState)
            ],
        )
        self.current_flos = 0
        self.hp_search_backend = None
        self.is_deepspeed_enabled = getattr(self.accelerator.state, "deepspeed_plugin", None) is not None
        self.is_fsdp_enabled = getattr(self.accelerator.state, "fsdp_plugin", None) is not None
        # Create distant repo and output directory if needed
        self.hub_model_id = None
        if self.args.push_to_hub:
            self.init_hf_repo()
        if self.args.should_save:
            os.makedirs(self.args.output_dir, exist_ok=True)

        # Add tags for models that have been loaded with the correct transformers version
        if hasattr(self.model, "add_model_tags"):
            self.model.add_model_tags(self._tag_names)

        #########
        # setup dataloader
        #########
        self.dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.local_dataloader_batch_size,
            shuffle=True,
            collate_fn=self.data_collator,
            drop_last=True,  # needed; otherwise the last batch will be of ragged shape
        )
        # sync random states for DataLoader(shuffle=True) before `accelerator.prepare`
        # see https://gist.github.com/vwxyzjn/2581bff1e48e185e0b85b6dfe1def79c
        torch.manual_seed(args.seed)
        self.model, self.optimizer, self.dataloader = accelerator.prepare(self.model, self.optimizer, self.dataloader)
        torch.manual_seed(self.local_seed)  # reset the local seed again

        self.eval_dataloader = DataLoader(
            self.eval_dataset,
            batch_size=args.per_device_eval_batch_size,
            collate_fn=self.data_collator,
            drop_last=True,
        )  # no need to shuffle eval dataset
        self.eval_dataloader = accelerator.prepare(self.eval_dataloader)

        if self.is_deepspeed_enabled:
            self.reward_model = prepare_deepspeed(
                self.reward_model, args.per_device_train_batch_size, args.fp16, args.bf16
            )

            if self.ref_model is None:
                if not self.is_peft_model:
                    raise ValueError("No reference model and model is not a Peft model.")
            else:
                self.ref_model = prepare_deepspeed(
                    self.ref_model, args.per_device_train_batch_size, args.fp16, args.bf16
                )
        else:
            if self.ref_model is None:
                if not self.is_peft_model:
                    raise ValueError("No reference model and model is not a Peft model.")
            else:
                self.ref_model = self.ref_model.to(self.accelerator.device)
            self.reward_model = self.reward_model.to(self.accelerator.device)

    def get_train_dataloader(self) -> DataLoader:
        return self.dataloader

    def get_eval_dataloader(self) -> DataLoader:
        return self.eval_dataloader

    @contextmanager
    def null_ref_context(self):
        """Context manager for handling null reference model (that is, peft adapter manipulation)."""
        with (
            self.accelerator.unwrap_model(self.model.policy).disable_adapter()
            if self.is_peft_model and not self.ref_adapter_name
            else nullcontext()
        ):
            if self.ref_adapter_name:
                self.model.policy.set_adapter(self.ref_adapter_name)
            yield
            if self.ref_adapter_name:
                self.model.policy.set_adapter(self.model_adapter_name or "default")

    def save_model(self, output_dir: str | None = None, _internal_call: bool = False):
        backup_model = self.model
        self.model = self.model.policy  # save only the policy

        if self.is_deepspeed_enabled:
            backup_deepspeed = self.deepspeed
            self.deepspeed = self.model

        super().save_model(output_dir, _internal_call)

        self.model = backup_model

        if self.is_deepspeed_enabled:
            self.deepspeed = backup_deepspeed

    def train(self):
        args = self.args
        accelerator = self.accelerator
        optimizer = self.optimizer
        model = self.model
        ref_policy = self.ref_model
        reward_model = self.reward_model
        processing_class = self.processing_class
        dataloader = self.dataloader
        device = accelerator.device

        def repeat_generator():
            while True:
                yield from dataloader

        iter_dataloader = iter(repeat_generator())
        generation_kwargs = {
            "max_new_tokens": args.response_length,
            "temperature": (args.temperature + 1e-7),
            "top_k": 0.0,
            "top_p": 1.0,
            "do_sample": True,
        }
        generation_config = GenerationConfig(**generation_kwargs)

        accelerator.print("===training policy===")
        start_time = time.time()
        stats_shape = (args.num_ppo_epochs, args.num_mini_batches, args.gradient_accumulation_steps)
        approxkl_stats = torch.zeros(stats_shape, device=device)
        pg_clipfrac_stats = torch.zeros(stats_shape, device=device)
        pg_loss_stats = torch.zeros(stats_shape, device=device)
        vf_loss_stats = torch.zeros(stats_shape, device=device)
        vf_clipfrac_stats = torch.zeros(stats_shape, device=device)
        entropy_stats = torch.zeros(stats_shape, device=device)
        ratio_stats = torch.zeros(stats_shape, device=device)
        model.train()

        # trainer state initialization
        self.state.global_step = 0
        self.state.episode = 0
        self.state.max_steps = args.num_total_batches
        self.state.num_train_epochs = args.total_episodes / self.train_dataset_len
        # Compute absolute values for logging, eval, and save if given as ratio
        if args.logging_steps is not None:
            if args.logging_steps < 1:
                self.state.logging_steps = math.ceil(self.state.max_steps * args.logging_steps)
            else:
                self.state.logging_steps = args.logging_steps
        if args.eval_steps is not None:
            if args.eval_steps < 1:
                self.state.eval_steps = math.ceil(self.state.max_steps * args.eval_steps)
            else:
                self.state.eval_steps = args.eval_steps
        if args.save_steps is not None:
            if args.save_steps < 1:
                self.state.save_steps = math.ceil(self.state.max_steps * args.save_steps)
            else:
                self.state.save_steps = args.save_steps
        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)

        # backward compatibility
        if self.is_deepspeed_enabled:
            self.deepspeed = self.model
            self.model_wrapped = self.model

        for update in range(1, args.num_total_batches + 1):
            self.state.episode += 1 * args.batch_size
            data = next(iter_dataloader)
            with torch.no_grad():
                queries = data["input_ids"].to(device)
                context_length = queries.shape[1]
                responses = []
                postprocessed_responses = []
                logprobs = []
                ref_logprobs = []
                scores = []
                sequence_lengths = []
                values = []
                with (
                    unwrap_model_for_generation(
                        self.model,
                        self.accelerator,
                        gather_deepspeed3_params=self.args.ds3_gather_for_generation,
                        generation_kwargs=generation_kwargs,  # Override model.generation_config with generation_kwargs to fix transformers#42762
                    ) as unwrapped_model
                ):
                    query_responses, logitss = batch_generation(
                        unwrapped_model.policy,
                        queries,
                        args.local_rollout_forward_batch_size,
                        processing_class.pad_token_id,
                        generation_config,
                    )

                for i in range(0, queries.shape[0], args.local_rollout_forward_batch_size):
                    query = queries[i : i + args.local_rollout_forward_batch_size]
                    query_response = query_responses[i : i + args.local_rollout_forward_batch_size]
                    response = query_response[:, context_length:]
                    logits = logitss[i : i + args.local_rollout_forward_batch_size]
                    logprob = selective_log_softmax(logits, response)
                    del logits
                    empty_cache()

                    if ref_policy is None:
                        with self.null_ref_context():
                            ref_output = forward(model.policy, query_response, processing_class.pad_token_id)
                    else:
                        ref_output = forward(ref_policy, query_response, processing_class.pad_token_id)
                    ref_logits = ref_output.logits[:, context_length - 1 : -1]
                    ref_logits /= args.temperature + 1e-7
                    ref_logprob = selective_log_softmax(ref_logits, response)
                    del ref_output, ref_logits
                    empty_cache()

                    # Response Processing 1. truncate response after the first occurrence of `stop_token_id`
                    postprocessed_response = response
                    if self.stop_token_id is not None:  # handle the edge case when stop_token_id exists but is 0
                        postprocessed_response = truncate_response(
                            self.stop_token_id, processing_class.pad_token_id, response
                        )

                    # Response Processing 2. run reward model on the truncated responses
                    postprocessed_query_response = torch.cat((query, postprocessed_response), 1)
                    sequence_length = first_true_indices(postprocessed_response == processing_class.pad_token_id) - 1
                    unwrapped_value_model = accelerator.unwrap_model(model).value_model
                    full_value, _, _ = get_reward(
                        unwrapped_value_model, query_response, processing_class.pad_token_id, context_length
                    )
                    value = full_value[:, context_length - 1 : -1].squeeze(-1)
                    _, score, _ = get_reward(
                        reward_model, postprocessed_query_response, processing_class.pad_token_id, context_length
                    )

                    responses.append(response)
                    postprocessed_responses.append(postprocessed_response)
                    logprobs.append(logprob)
                    ref_logprobs.append(ref_logprob)
                    sequence_lengths.append(sequence_length)
                    scores.append(score)
                    values.append(value)
                responses = torch.cat(responses, 0)
                postprocessed_responses = torch.cat(postprocessed_responses, 0)
                logprobs = torch.cat(logprobs, 0)
                ref_logprobs = torch.cat(ref_logprobs, 0)
                sequence_lengths = torch.cat(sequence_lengths, 0)
                scores = torch.cat(scores, 0)
                values = torch.cat(values, 0)
                del (logprob, ref_logprob, full_value, value, score, unwrapped_model)
                empty_cache()
                gc.collect()

                # Response Processing 3. Filter completion. Ensure that the sample contains stop_token_id
                # Completions not passing that filter will receive a lower score.
                contain_eos_token = torch.any(postprocessed_responses == self.processing_class.eos_token_id, dim=-1)
                if self.args.missing_eos_penalty is not None:
                    scores[~contain_eos_token] -= self.args.missing_eos_penalty
                # accelerator.print(f"{scores=}, {(contain_eos_token.sum() / len(contain_eos_token))=}")

                # be very careful with `padding_mask_p1`; see https://excalidraw.com/#json=LWnzG4w2k5DjF_EOL_xPt,e2w3a-hFJ_gX5vOfeyXGTw
                response_idxs = torch.arange(responses.shape[1], device=responses.device).repeat(responses.shape[0], 1)
                padding_mask = response_idxs > sequence_lengths.unsqueeze(1)
                logprobs = torch.masked_fill(logprobs, padding_mask, INVALID_LOGPROB)
                ref_logprobs = torch.masked_fill(ref_logprobs, padding_mask, INVALID_LOGPROB)
                sequence_lengths_p1 = sequence_lengths + 1
                padding_mask_p1 = response_idxs > (sequence_lengths_p1.unsqueeze(1))
                values = torch.masked_fill(values, padding_mask_p1, 0)

                # 4. compute rewards
                # Formula used by http://joschu.net/blog/kl-approx.html for the k1 and k3 estimators
                logr = ref_logprobs - logprobs
                kl = -logr if args.kl_estimator == "k1" else (logr.exp() - 1) - logr  # Else statement is k3
                non_score_reward = -args.kl_coef * kl
                rewards = non_score_reward.clone()
                actual_start = torch.arange(rewards.size(0), device=rewards.device)
                actual_end = torch.where(sequence_lengths_p1 < rewards.size(1), sequence_lengths_p1, sequence_lengths)
                rewards[actual_start, actual_end] += scores

                # 5. whiten rewards
                if args.whiten_rewards:
                    rewards = masked_whiten(rewards, mask=~padding_mask_p1, shift_mean=False)
                    rewards = torch.masked_fill(rewards, padding_mask_p1, 0)

                # 6. compute advantages and returns
                lastgaelam = 0
                advantages_reversed = []
                gen_length = responses.shape[1]
                for t in reversed(range(gen_length)):
                    nextvalues = values[:, t + 1] if t < gen_length - 1 else 0.0
                    delta = rewards[:, t] + args.gamma * nextvalues - values[:, t]
                    lastgaelam = delta + args.gamma * args.lam * lastgaelam
                    advantages_reversed.append(lastgaelam)
                advantages = torch.stack(advantages_reversed[::-1], axis=1)
                returns = advantages + values
                advantages = masked_whiten(advantages, ~padding_mask)
                advantages = torch.masked_fill(advantages, padding_mask, 0)
                empty_cache()

            # Do multiple epochs of PPO training, with a fresh random shuffle in each epoch
            for ppo_epoch_idx in range(args.num_ppo_epochs):
                b_inds = np.random.permutation(args.local_batch_size)
                minibatch_idx = 0
                for mini_batch_start in range(0, args.local_batch_size, args.local_mini_batch_size):
                    mini_batch_end = mini_batch_start + args.local_mini_batch_size
                    mini_batch_inds = b_inds[mini_batch_start:mini_batch_end]
                    gradient_accumulation_idx = 0
                    for micro_batch_start in range(0, args.local_mini_batch_size, args.per_device_train_batch_size):
                        with accelerator.accumulate(model):
                            micro_batch_end = micro_batch_start + args.per_device_train_batch_size
                            micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]
                            mb_advantage = advantages[micro_batch_inds]
                            mb_responses = responses[micro_batch_inds]
                            mb_query_responses = query_responses[micro_batch_inds]
                            mb_logprobs = logprobs[micro_batch_inds]
                            mb_return = returns[micro_batch_inds]
                            mb_values = values[micro_batch_inds]

                            output, vpred_temp = forward(model, mb_query_responses, processing_class.pad_token_id)
                            logits = output.logits[:, context_length - 1 : -1]
                            logits /= args.temperature + 1e-7
                            new_logprobs = selective_log_softmax(logits, mb_responses)
                            new_logprobs = torch.masked_fill(
                                new_logprobs, padding_mask[micro_batch_inds], INVALID_LOGPROB
                            )
                            vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)
                            vpred = torch.masked_fill(vpred, padding_mask_p1[micro_batch_inds], 0)
                            vpredclipped = torch.clamp(
                                vpred,
                                mb_values - args.cliprange_value,
                                mb_values + args.cliprange_value,
                            )
                            vf_losses1 = torch.square(vpred - mb_return)
                            vf_losses2 = torch.square(vpredclipped - mb_return)
                            vf_loss_max = torch.max(vf_losses1, vf_losses2)
                            vf_loss = 0.5 * masked_mean(vf_loss_max, ~padding_mask_p1[micro_batch_inds])
                            vf_clipfrac = masked_mean(
                                (vf_losses2 > vf_losses1).float(), ~padding_mask_p1[micro_batch_inds]
                            )
                            logprobs_diff = new_logprobs - mb_logprobs
                            ratio = torch.exp(logprobs_diff)
                            pg_losses = -mb_advantage * ratio
                            pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)
                            pg_loss_max = torch.max(pg_losses, pg_losses2)
                            pg_loss = masked_mean(pg_loss_max, ~padding_mask[micro_batch_inds])
                            loss = pg_loss + args.vf_coef * vf_loss
                            accelerator.backward(loss)
                            optimizer.step()
                            optimizer.zero_grad()
                            with torch.no_grad():
                                pg_clipfrac = masked_mean(
                                    (pg_losses2 > pg_losses).float(), ~padding_mask[micro_batch_inds]
                                )
                                prob_dist = torch.nn.functional.softmax(logits, dim=-1)
                                entropy = torch.logsumexp(logits, dim=-1) - torch.sum(prob_dist * logits, dim=-1)
                                approxkl = 0.5 * (logprobs_diff**2).mean()
                                approxkl_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = approxkl
                                pg_clipfrac_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = (
                                    pg_clipfrac
                                )
                                pg_loss_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = pg_loss
                                vf_loss_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = vf_loss
                                vf_clipfrac_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = (
                                    vf_clipfrac
                                )
                                entropy_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = entropy.mean()
                                ratio_stats[ppo_epoch_idx, minibatch_idx, gradient_accumulation_idx] = ratio.mean()
                        gradient_accumulation_idx += 1
                    minibatch_idx += 1
                    # del everything and empty cache
                    # fmt: off
                    del (
                        output, vpred_temp, logits, new_logprobs, vpred, vpredclipped,
                        vf_losses1, vf_losses2, vf_loss, vf_clipfrac, logprobs_diff, ratio, pg_losses, pg_losses2, pg_loss_max,
                        pg_loss, loss, pg_clipfrac, prob_dist, entropy, approxkl, mb_return,
                        mb_advantage, mb_values, mb_responses, mb_query_responses, mb_logprobs,
                    )
                    # fmt: on
                    empty_cache()
            with torch.no_grad():
                mean_kl = kl.sum(1).mean()
                mean_entropy = (-logprobs).sum(1).mean()
                mean_non_score_reward = non_score_reward.sum(1).mean()
                rlhf_reward = mean_non_score_reward + scores.mean()
                eps = int(self.state.episode / (time.time() - start_time))
                metrics = {}
                metrics["eps"] = eps
                metrics["objective/kl"] = self.accelerator.gather_for_metrics(mean_kl).mean().item()
                metrics["objective/entropy"] = self.accelerator.gather_for_metrics(mean_entropy).mean().item()
                metrics["objective/non_score_reward"] = (
                    self.accelerator.gather_for_metrics(mean_non_score_reward).mean().item()
                )
                metrics["objective/rlhf_reward"] = self.accelerator.gather_for_metrics(rlhf_reward).mean().item()
                metrics["objective/scores"] = self.accelerator.gather_for_metrics(scores.mean()).mean().item()
                metrics["policy/approxkl_avg"] = self.accelerator.gather_for_metrics(approxkl_stats).mean().item()
                metrics["policy/clipfrac_avg"] = self.accelerator.gather_for_metrics(pg_clipfrac_stats).mean().item()
                metrics["loss/policy_avg"] = self.accelerator.gather_for_metrics(pg_loss_stats).mean().item()
                metrics["loss/value_avg"] = self.accelerator.gather_for_metrics(vf_loss_stats).mean().item()
                metrics["val/clipfrac_avg"] = self.accelerator.gather_for_metrics(vf_clipfrac_stats).mean().item()
                metrics["policy/entropy_avg"] = self.accelerator.gather_for_metrics(entropy_stats).mean().item()
                metrics["val/ratio"] = self.accelerator.gather_for_metrics(ratio_stats).mean().item()
                metrics["val/ratio_var"] = self.accelerator.gather_for_metrics(ratio_stats).var().item()
                metrics["val/num_eos_tokens"] = (responses == processing_class.eos_token_id).sum().item()
                metrics["lr"] = self.lr_scheduler.get_last_lr()[0]
                metrics["episode"] = self.state.episode
                self.state.epoch = self.state.episode / self.train_dataset_len  # used by self.log
                self.state.global_step += 1
                self.log(metrics)

            self.lr_scheduler.step()
            self.control = self.callback_handler.on_step_end(args, self.state, self.control)
            if self.control.should_save:
                self._save_checkpoint(model, trial=None)
                self.control = self.callback_handler.on_save(self.args, self.state, self.control)
            del kl, mean_kl, mean_entropy, mean_non_score_reward, scores, metrics, non_score_reward
            empty_cache()
            gc.collect()

            if args.num_sample_generations > 0 and (update - 1) % self.sample_generations_freq == 0:
                self.generate_completions(sampling=True)
                empty_cache()
            del (
                query_responses,
                responses,
                postprocessed_responses,
                logprobs,
                ref_logprobs,
                values,
                sequence_lengths,
                contain_eos_token,
                sequence_lengths_p1,
                response_idxs,
                padding_mask,
                padding_mask_p1,
                rewards,
                actual_start,
                actual_end,
                advantages,
                returns,
            )
            empty_cache()

        # HF trainer specifics
        self.control = self.callback_handler.on_train_end(args, self.state, self.control)
        if self.control.should_save:
            self._save_checkpoint(model, trial=None)
            self.control = self.callback_handler.on_save(self.args, self.state, self.control)

    def generate_completions(self, sampling: bool = False):
        args = self.args
        processing_class = self.processing_class
        generation_kwargs = {
            "max_new_tokens": args.response_length,
            "temperature": (0.01 + 1e-7),
            "top_k": 0.0,
            "top_p": 1.0,
            "do_sample": True,
        }
        generation_config = GenerationConfig(**generation_kwargs)

        table = defaultdict(list)
        with (
            unwrap_model_for_generation(
                self.model,
                self.accelerator,
                gather_deepspeed3_params=self.args.ds3_gather_for_generation,
                generation_kwargs=generation_kwargs,  # Override model.generation_config with generation_kwargs to fix transformers#42762
            ) as unwrapped_model
        ):
            for batch in self.eval_dataloader:
                query = batch["input_ids"]
                with torch.no_grad():
                    context_length = query.shape[1]
                    query_response, _ = batch_generation(
                        unwrapped_model.policy,
                        query,
                        query.shape[0],
                        processing_class.pad_token_id,
                        generation_config,
                    )
                    response = query_response[:, context_length:]
                    postprocessed_response = response
                    if self.stop_token_id is not None:  # handle the edge case when stop_token_id exists but is 0
                        postprocessed_response = truncate_response(
                            self.stop_token_id, processing_class.pad_token_id, response
                        )
                    table["query"].extend(
                        gather_object(processing_class.batch_decode(query, skip_special_tokens=True))
                    )
                    table["model response"].extend(
                        gather_object(processing_class.batch_decode(postprocessed_response))
                    )

                    postprocessed_query_response = torch.cat((query, postprocessed_response), 1)
                    _, score, _ = get_reward(
                        self.reward_model, postprocessed_query_response, processing_class.pad_token_id, context_length
                    )
                    table["score"].extend(self.accelerator.gather_for_metrics(score).float().cpu().numpy())

                if sampling:
                    break
        df = pd.DataFrame(table)

        if self.accelerator.is_main_process:
            if is_rich_available():
                print_rich_table(df.iloc[0 : 0 + 5])
            if "wandb" in args.report_to:
                import wandb

                if wandb.run is not None:
                    wandb.log({"completions": wandb.Table(dataframe=df)})

            if "comet_ml" in args.report_to:
                log_table_to_comet_experiment(
                    name="completions.csv",
                    table=df,
                )

    # Ensure the model card is saved along with the checkpoint
    def _save_checkpoint(self, model, trial):
        if self.args.hub_model_id is None:
            model_name = Path(self.args.output_dir).name
        else:
            model_name = self.args.hub_model_id.split("/")[-1]
        self.create_model_card(model_name=model_name)
        super()._save_checkpoint(model, trial)

```