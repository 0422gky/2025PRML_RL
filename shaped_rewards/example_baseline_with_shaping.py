"""
å¸¦ Reward Shaping çš„ PPO è®­ç»ƒè„šæœ¬

åŸºäº baseline.pyï¼Œä½¿ç”¨ shaped_rewards ä¸­çš„ PPOTrainer å®ç°é•¿åº¦æƒ©ç½šå’Œé‡å¤æƒ©ç½š
"""
import os
import sys

# ç¡®ä¿ä¼˜å…ˆä½¿ç”¨æœ¬åœ°çš„ trl åŒ…ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
workspace_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
trl_path = os.path.join(workspace_root, "trl")
if os.path.exists(trl_path):
    sys.path.insert(0, trl_path)

# æ·»åŠ  shaped_rewards ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ä¿®æ”¹åçš„ trainer
sys.path.insert(0, os.path.dirname(__file__))

# å¯¼å…¥ä¿®æ”¹åçš„ trainerï¼ˆå¸¦ reward shapingï¼‰
from ppo_trainer_shaped import PPOTrainer
from trl.experimental.ppo import PPOConfig  # ä½¿ç”¨åŸå§‹çš„ PPOConfig

import torch
from accelerate import PartialState
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    BitsAndBytesConfig,
    AutoConfig,
)
from peft import PeftModel

# ä¿®å¤ wandb JSON decode é”™è¯¯
# æ–¹æ¡ˆ 1: ä½¿ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆé¿å…ç½‘ç»œé—®é¢˜ï¼‰
os.environ.setdefault("WANDB_MODE", "offline")

# æ–¹æ¡ˆ 2: å¦‚æœéœ€è¦åœ¨çº¿æ¨¡å¼ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶è®¾ç½® API key
# os.environ["WANDB_API_KEY"] = "your-api-key"
# os.environ["WANDB_PROJECT"] = "your-project-name"

# æ–¹æ¡ˆ 3: å¦‚æœä»ç„¶æœ‰é—®é¢˜ï¼Œå¯ä»¥åœ¨è®­ç»ƒé…ç½®ä¸­è®¾ç½® report_to="none" æ¥ç¦ç”¨ wandb

# æ ¹æ®trl_sftåŠ å…¥äº†RLHFä¹‹åçš„æ¨¡å‹
# -------------------------
# 0) paths
# -------------------------
base_model_name = "Qwen/Qwen2.5-0.5B-Instruct"

# SFTè®­ç»ƒåçš„adapterä¿å­˜åˆ°è¿™ä¸ªç›®å½•,ä»è¿™é‡ŒåŠ è½½LoRAå¾®è°ƒçš„SFT
# æ³¨æ„ï¼šè·¯å¾„ç›¸å¯¹äºè¿è¡Œè„šæœ¬çš„ä½ç½®ï¼Œä» shaped_rewards ç›®å½•è¿è¡Œæ—¶ä½¿ç”¨ ../trl_sft
# å¦‚æœä» workspace æ ¹ç›®å½•è¿è¡Œï¼Œä½¿ç”¨ ./trl_sft
script_dir = os.path.dirname(os.path.abspath(__file__))
workspace_root = os.path.dirname(script_dir)  # è·å– workspace æ ¹ç›®å½•

sft_adapter_dir = os.path.join(workspace_root, "trl_sft", "sft_tldr_lora", "checkpoint-63")

# Reward model è·¯å¾„
# é€‰é¡¹ 1: ä½¿ç”¨è‡ªå·±è®­ç»ƒçš„ reward modelï¼ˆæ¨èï¼Œä¸ Qwen tokenizer å…¼å®¹ï¼‰
reward_model_path = os.path.join(workspace_root, "trl_sft", "reward_model", "Qwen2.5-0.5B-Instruct-Reward")

# é€‰é¡¹ 2: ä½¿ç”¨å…¬å¼€çš„ reward modelï¼ˆæ³¨æ„ï¼štokenizer å¯èƒ½ä¸å…¼å®¹ï¼‰
# reward_model_path = "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr"

# å¦‚æœè®­ç»ƒå¥½çš„ reward model ä¸å­˜åœ¨ï¼Œå¯ä»¥ä½¿ç”¨å…¬å¼€æ¨¡å‹ä½œä¸ºå¤‡é€‰
if not os.path.exists(reward_model_path) or not os.listdir(reward_model_path):
    print(f"âš ï¸  è­¦å‘Š: {reward_model_path} ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°†ä½¿ç”¨å…¬å¼€æ¨¡å‹")
    reward_model_path = "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr"

# -------------------------
# 1) tokenizer
# -------------------------
tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side="left")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

bnb = BitsAndBytesConfig(load_in_8bit=True)

# -------------------------
# 2) policy (trainable): Qwen base + your LoRA adapter
# -------------------------
base_policy = AutoModelForCausalLM.from_pretrained(
    base_model_name, device_map="auto", quantization_config=bnb, trust_remote_code=True
)
policy_model = PeftModel.from_pretrained(base_policy, sft_adapter_dir, is_trainable=True)

# -------------------------
# 3) ref (frozen): same init as policy, but frozen
# -------------------------
base_ref = AutoModelForCausalLM.from_pretrained(
    base_model_name, device_map="auto", quantization_config=bnb, trust_remote_code=True
)
ref_model = PeftModel.from_pretrained(base_ref, sft_adapter_dir, is_trainable=False)
ref_model.eval()
for p in ref_model.parameters():
    p.requires_grad = False

# -------------------------
# 4) reward model and value model (frozen)
# -------------------------
# ä½¿ç”¨è‡ªå·±è®­ç»ƒçš„ reward modelï¼ˆåŸºäº Qwenï¼Œtokenizer å…¼å®¹ï¼‰
print(f"ğŸ“¦ åŠ è½½ reward model from: {reward_model_path}")

# å…ˆåŠ è½½ config æ¥æ£€æŸ¥ num_labels
try:
    reward_config = AutoConfig.from_pretrained(reward_model_path, trust_remote_code=True)
    # å¦‚æœ config ä¸­æœ‰ num_labelsï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™é»˜è®¤ä½¿ç”¨ 1
    num_labels = getattr(reward_config, 'num_labels', 1)
    print(f"   æ£€æµ‹åˆ° num_labels: {num_labels}")
except Exception as e:
    print(f"   âš ï¸  æ— æ³•è¯»å– configï¼Œä½¿ç”¨é»˜è®¤ num_labels=1: {e}")
    num_labels = 1

# åŠ è½½ reward model
# æ³¨æ„ï¼šå¦‚æœ checkpoint ä¸­çš„ num_labels ä¸æŒ‡å®šå€¼ä¸åŒ¹é…ï¼Œä¼šæŠ¥é”™
# è§£å†³æ–¹æ¡ˆï¼šä¸æŒ‡å®š num_labelsï¼Œè®©æ¨¡å‹ä» checkpoint ä¸­è‡ªåŠ¨è¯»å–
reward_model = AutoModelForSequenceClassification.from_pretrained(
    reward_model_path, 
    # ä¸æŒ‡å®š num_labelsï¼Œè®©æ¨¡å‹ä» checkpoint çš„ config ä¸­è¯»å–
    device_map="auto", 
    trust_remote_code=True,
)
reward_model.eval()
for p in reward_model.parameters():
    p.requires_grad = False

# value_model é€šå¸¸å’Œ reward_model ç›¸åŒ
value_model = AutoModelForSequenceClassification.from_pretrained(
    reward_model_path, 
    # ä¸æŒ‡å®š num_labelsï¼Œè®©æ¨¡å‹ä» checkpoint ä¸­è‡ªåŠ¨è¯»å–
    device_map="auto", 
    trust_remote_code=True,
)
value_model.eval()
for p in value_model.parameters():
    p.requires_grad = False

# å¦‚æœ num_labels=2ï¼Œéœ€è¦åˆ›å»ºåŒ…è£…å™¨æ¥é€‚é… PPO trainer çš„æœŸæœ›
# PPO trainer æœŸæœ› reward/value model è¾“å‡ºå½¢çŠ¶ä¸º [batch, seq] æˆ– [batch, seq, 1]
if num_labels == 2:
    print("   âš ï¸  æ£€æµ‹åˆ° num_labels=2ï¼Œåˆ›å»ºåŒ…è£…å™¨ä»¥é€‚é… PPO trainer")
    
    class RewardModelWrapper(torch.nn.Module):
        """åŒ…è£…å™¨ï¼šå°† num_labels=2 çš„è¾“å‡ºè½¬æ¢ä¸º num_labels=1 çš„è¾“å‡º"""
        def __init__(self, model):
            super().__init__()
            self.model = model
            self.base_model_prefix = model.base_model_prefix
            
        def score(self, hidden_states):
            """åªå–ç¬¬ä¸€ä¸ªç»´åº¦çš„è¾“å‡ºï¼ˆæˆ–è€…å¯ä»¥å–å¹³å‡ï¼Œæ ¹æ®è®­ç»ƒæ–¹å¼ï¼‰"""
            scores = self.model.score(hidden_states)  # [batch, seq, 2]
            # å–ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œæˆ–è€…æ ¹æ®è®­ç»ƒæ–¹å¼è°ƒæ•´
            # å¦‚æœæ˜¯ preference learningï¼Œå¯èƒ½éœ€è¦å– chosen - rejected
            # è¿™é‡Œå‡è®¾ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯ reward score
            return scores[..., 0:1]  # ä¿æŒç»´åº¦ [batch, seq, 1]ï¼Œè¿™æ · squeeze(-1) åæ˜¯ [batch, seq]
        
        def __getattr__(self, name):
            """è½¬å‘å…¶ä»–å±æ€§åˆ°åŸå§‹æ¨¡å‹"""
            try:
                return super().__getattr__(name)
            except AttributeError:
                return getattr(self.model, name)
    
    # åŒ…è£… reward model å’Œ value model
    reward_model = RewardModelWrapper(reward_model)
    value_model = RewardModelWrapper(value_model)
    print("   âœ… Reward model å’Œ Value model åŒ…è£…å®Œæˆ")

print("âœ… Reward model å’Œ value model åŠ è½½å®Œæˆ")

# -------------------------
# 5) TL;DR dataset -> PPO queries
# -------------------------
raw_dataset = load_dataset("trl-lib/tldr", split="train[:1000]")

def prepare_dataset(dataset, tokenizer):
    """pre-tokenize the dataset before training; only collate during training"""

    def tokenize(element):
        input_ids = tokenizer(element["prompt"], padding=False)["input_ids"]
        return {"input_ids": input_ids, "lengths": len(input_ids)}

    return dataset.map(
        tokenize,
        remove_columns=dataset.column_names,
    )

# Compute that only on the main process for faster data processing.
with PartialState().local_main_process_first():
    train_dataset = prepare_dataset(raw_dataset, tokenizer)
    # filtering
    train_dataset = train_dataset.filter(lambda x: x["lengths"] <= 512)

assert train_dataset[0]["input_ids"][-1] != tokenizer.eos_token_id, "The last token should not be an EOS token"

# åˆ›å»ºä¸€ä¸ªå°çš„ eval_datasetï¼ˆç”¨äº generate_completionsï¼Œå³ä½¿ eval_strategy="no"ï¼‰
# PPO trainer éœ€è¦ eval_dataset æ¥ç”Ÿæˆç¤ºä¾‹ï¼Œå³ä½¿ä¸ä½¿ç”¨è¯„ä¼°
eval_dataset = train_dataset.select(range(min(10, len(train_dataset))))

# -------------------------
# 6) PPO config with Reward Shaping
# -------------------------
training_args = PPOConfig(
    output_dir="./ppo_tldr_rlhf_shaped",
    learning_rate=1e-5,
    
    per_device_train_batch_size=4,      # æ¯ä¸ªè®¾å¤‡çš„batch size
    gradient_accumulation_steps=4,       # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    local_rollout_forward_batch_size=16, # rollouté˜¶æ®µçš„forward batch size
    num_ppo_epochs=4,                    # PPO epochs
    
    # ç”Ÿæˆå‚æ•°
    response_length=128,                # å“åº”é•¿åº¦
    temperature=1.0,                     # é‡‡æ ·æ¸©åº¦
    stop_token="eos",                    # åœæ­¢token
    
    kl_coef=0.1,                        # KLæ•£åº¦ç³»æ•°
    
    # å…¶ä»–å‚æ•°
    # å¦‚æœ wandb ä»æœ‰é—®é¢˜ï¼Œå¯ä»¥æ”¹ä¸º "none" æˆ– [] æ¥ç¦ç”¨
    report_to="wandb",  # æˆ– "none" æ¥ç¦ç”¨ wandb
    eval_strategy="no",                  # ä¸ä½¿ç”¨è¯„ä¼°
    num_sample_generations=0,            # ç¦ç”¨ generate_completionsï¼ˆéœ€è¦ eval_datasetï¼‰
    
    # Reward Shaping å‚æ•°ï¼ˆå¯é€‰ï¼Œæœ‰é»˜è®¤å€¼ï¼‰
    # æ³¨æ„ï¼šè¿™äº›å‚æ•°ä¸åœ¨ PPOConfig çš„æ­£å¼å®šä¹‰ä¸­ï¼Œä½†å¯ä»¥é€šè¿‡åŠ¨æ€å±æ€§è®¾ç½®
    # å¦‚æœä¸åœ¨ PPOConfig ä¸­è®¾ç½®ï¼Œä»£ç ä¼šä½¿ç”¨é»˜è®¤å€¼
    # length_penalty_coef=0.01,        # é•¿åº¦æƒ©ç½šç³»æ•°ï¼Œé»˜è®¤ 0.01
    # repetition_penalty_coef=0.1,     # é‡å¤æƒ©ç½šç³»æ•°ï¼Œé»˜è®¤ 0.1
    # repetition_n_gram=3,             # n-gram å¤§å°ï¼Œé»˜è®¤ 3
)

# å¦‚æœéœ€è¦åœ¨ PPOConfig ä¸­è®¾ç½® reward shaping å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ setattr
# æˆ–è€…ç›´æ¥åœ¨åˆ›å»ºæ—¶ä¼ é€’ï¼ˆè™½ç„¶ PPOConfig ä¸æ­£å¼æ”¯æŒï¼Œä½†å¯ä»¥é€šè¿‡ **kwargs ä¼ é€’ï¼‰
# è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨é»˜è®¤å€¼ï¼Œå¦‚æœéœ€è¦è‡ªå®šä¹‰ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼š
# training_args.length_penalty_coef = 0.01      # é•¿åº¦æƒ©ç½šç³»æ•°
# training_args.repetition_penalty_coef = 0.1   # é‡å¤æƒ©ç½šç³»æ•°
# training_args.repetition_n_gram = 3           # n-gram å¤§å°

# -------------------------
# 7) PPO Trainer (å¸¦ Reward Shaping)
# -------------------------
# ä½¿ç”¨ä¿®æ”¹åçš„ PPOTrainerï¼Œè‡ªåŠ¨åº”ç”¨é•¿åº¦æƒ©ç½šå’Œé‡å¤æƒ©ç½š
trainer = PPOTrainer(
    args=training_args,
    processing_class=tokenizer,
    model=policy_model,
    ref_model=ref_model,
    reward_model=reward_model,
    value_model=value_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,  # æä¾› eval_dataset ç”¨äº generate_completions
    peft_config=None,  # å·²ç»åŠ è½½äº†PeftModelï¼Œä¸éœ€è¦peft_config
)

# -------------------------
# 8) å¼€å§‹è®­ç»ƒ
# -------------------------
print("ğŸš€ å¼€å§‹ PPO è®­ç»ƒï¼ˆå¸¦ Reward Shapingï¼‰...")
trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model(training_args.output_dir)
print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {training_args.output_dir}")

print("âœ… è®­ç»ƒå®Œæˆï¼")
