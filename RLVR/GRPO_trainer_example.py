"""
RLVR (Reinforcement Learning with Verifiable Rewards) è®­ç»ƒè„šæœ¬

åŸºäº baseline.pyï¼Œä½¿ç”¨ GRPO æ–¹æ³•ï¼ˆä¸ä¾èµ– reward modelï¼‰
ä½¿ç”¨è‡ªå®šä¹‰ reward function æ¥æä¾›å¯éªŒè¯çš„å¥–åŠ±

æ³¨æ„ï¼šè™½ç„¶trlç»™å‡ºçš„å‚è€ƒä»£ç æ˜¯ PAPO_trainer_example.pyï¼Œä½†è¿™é‡Œä½¿ç”¨ GRPOï¼Œ
å› ä¸º PAPO ä¸»è¦ç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ï¼Œè€Œ TL;DR æ˜¯æ–‡æœ¬ä»»åŠ¡ã€‚
å¦‚æœéœ€è¦å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨ PAPOTrainerã€‚
"""
import os
import torch
from accelerate import PartialState
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import PeftModel
from trl import GRPOTrainer, GRPOConfig

# ä¿®å¤ wandb JSON decode é”™è¯¯
os.environ.setdefault("WANDB_MODE", "offline")

# -------------------------
# 0) paths
# -------------------------
base_model_name = "Qwen/Qwen2.5-0.5B-Instruct"

# SFTè®­ç»ƒåçš„adapterä¿å­˜åˆ°è¿™ä¸ªç›®å½•,ä»è¿™é‡ŒåŠ è½½LoRAå¾®è°ƒçš„SFT
# ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿ä»ä»»ä½•ä½ç½®è¿è¡Œéƒ½èƒ½æ‰¾åˆ°
script_dir = os.path.dirname(os.path.abspath(__file__))
workspace_root = os.path.dirname(script_dir)
sft_adapter_dir = os.path.join(workspace_root, "trl_sft", "sft_tldr_lora", "checkpoint-63")

# -------------------------
# 1) tokenizer
# -------------------------
tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side="left")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

bnb = BitsAndBytesConfig(load_in_8bit=True)

# -------------------------
# 2) policy (trainable): Qwen base + your LoRA adapter
# -------------------------
base_policy = AutoModelForCausalLM.from_pretrained(
    base_model_name, device_map="auto", quantization_config=bnb, trust_remote_code=True
)
policy_model = PeftModel.from_pretrained(base_policy, sft_adapter_dir, is_trainable=True)

# -------------------------
# 3) æ•°æ®é›†å‡†å¤‡
# -------------------------
raw_dataset = load_dataset("trl-lib/tldr", split="train[:1000]")

def prepare_dataset(dataset, tokenizer):
    """pre-tokenize the dataset before training; only collate during training"""

    def tokenize(element):
        # GRPO éœ€è¦ "prompt" åˆ—
        # TL;DR æ•°æ®é›†å·²ç»æœ‰ "prompt" å’Œ "label" åˆ—
        return {
            "prompt": element["prompt"],
            "label": element.get("label", ""),  # ä¿å­˜ ground truth ç”¨äº reward function
        }

    return dataset.map(
        tokenize,
        remove_columns=[col for col in dataset.column_names if col not in ["prompt", "label"]],
    )

# Compute that only on the main process for faster data processing.
with PartialState().local_main_process_first():
    train_dataset = prepare_dataset(raw_dataset, tokenizer)
    # è¿‡æ»¤å¤ªé•¿çš„ promptï¼ˆå¯é€‰ï¼‰
    # train_dataset = train_dataset.filter(lambda x: len(tokenizer(x["prompt"], padding=False)["input_ids"]) <= 512)

# åˆ›å»ºä¸€ä¸ªå°çš„ eval_dataset
eval_dataset = train_dataset.select(range(min(10, len(train_dataset))))

# -------------------------
# 4) è‡ªå®šä¹‰ Reward Function (RLVR)
# -------------------------
def tldr_reward_func(completions, label=None, **kwargs):
    """
    è‡ªå®šä¹‰ reward function for TL;DR ä»»åŠ¡
    
    Args:
        completions: ç”Ÿæˆçš„æ‘˜è¦åˆ—è¡¨
        label: ground truth æ‘˜è¦åˆ—è¡¨ï¼ˆæ¥è‡ªæ•°æ®é›†ï¼‰
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        rewards: æ¯ä¸ª completion çš„å¥–åŠ±å€¼åˆ—è¡¨
    """
    rewards = []
    
    for i, completion in enumerate(completions):
        # è·å–å¯¹åº”çš„ ground truth
        gt = label[i] if label and i < len(label) else None
        
        if gt:
            # ç®€å•çš„å¥–åŠ±ç­–ç•¥ï¼šåŸºäºä¸ ground truth çš„ç›¸ä¼¼åº¦
            # è¿™é‡Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…ï¼Œä½ å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æŒ‡æ ‡ï¼ˆå¦‚ ROUGE, BLEU ç­‰ï¼‰
            completion_text = completion if isinstance(completion, str) else completion.get("content", "")
            gt_text = gt if isinstance(gt, str) else gt.get("content", "")
            
            # ç®€å•çš„å¥–åŠ±è®¡ç®—ï¼š
            # 1. å¦‚æœ completion åŒ…å«å…³é”®ä¿¡æ¯ï¼Œç»™äºˆå¥–åŠ±
            # 2. é•¿åº¦æƒ©ç½šï¼ˆé¼“åŠ±ç®€æ´ï¼‰
            # 3. ä¸ ground truth çš„ç›¸ä¼¼åº¦
            
            # åŸºç¡€å¥–åŠ±ï¼šå¦‚æœ completion ä¸ä¸ºç©º
            reward = 0.1 if completion_text.strip() else -0.5
            
            # é•¿åº¦å¥–åŠ±ï¼šé¼“åŠ±åˆç†çš„é•¿åº¦ï¼ˆ50-200 å­—ç¬¦ï¼‰
            length = len(completion_text)
            if 50 <= length <= 200:
                reward += 0.2
            elif length > 200:
                reward -= 0.1 * (length - 200) / 100  # è¿‡é•¿æƒ©ç½š
            elif length < 50:
                reward -= 0.1 * (50 - length) / 50  # è¿‡çŸ­æƒ©ç½š
            
            # ç›¸ä¼¼åº¦å¥–åŠ±ï¼ˆç®€å•ç‰ˆæœ¬ï¼šåŸºäºå…±åŒè¯æ±‡ï¼‰
            if gt_text:
                completion_words = set(completion_text.lower().split())
                gt_words = set(gt_text.lower().split())
                if len(gt_words) > 0:
                    overlap = len(completion_words & gt_words) / len(gt_words)
                    reward += 0.7 * overlap  # ç›¸ä¼¼åº¦è´¡çŒ®æœ€å¤§
            
            rewards.append(reward)
        else:
            # å¦‚æœæ²¡æœ‰ ground truthï¼Œä½¿ç”¨ç®€å•çš„å¯å‘å¼å¥–åŠ±
            completion_text = completion if isinstance(completion, str) else completion.get("content", "")
            length = len(completion_text)
            
            # åŸºç¡€å¥–åŠ±
            reward = 0.1 if completion_text.strip() else -0.5
            
            # é•¿åº¦å¥–åŠ±
            if 50 <= length <= 200:
                reward += 0.3
            elif length > 200:
                reward -= 0.1 * (length - 200) / 100
            
            rewards.append(reward)
    
    return rewards


# å¯é€‰ï¼šä½¿ç”¨æ›´å¤æ‚çš„å¥–åŠ±å‡½æ•°ï¼ˆéœ€è¦å®‰è£…é¢å¤–ä¾èµ–ï¼‰
# ä¾‹å¦‚ä½¿ç”¨ ROUGE åˆ†æ•°ï¼š
# try:
#     from rouge_score import rouge_scorer
#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
#     
#     def tldr_reward_func_with_rouge(completions, label=None, **kwargs):
#         rewards = []
#         for i, completion in enumerate(completions):
#             completion_text = completion if isinstance(completion, str) else completion.get("content", "")
#             gt = label[i] if label and i < len(label) else None
#             
#             if gt:
#                 gt_text = gt if isinstance(gt, str) else gt.get("content", "")
#                 scores = scorer.score(gt_text, completion_text)
#                 # ä½¿ç”¨ ROUGE-L F1 åˆ†æ•°ä½œä¸ºä¸»è¦å¥–åŠ±
#                 reward = scores['rougeL'].fmeasure
#                 rewards.append(reward)
#             else:
#                 rewards.append(0.0)
#         return rewards
# except ImportError:
#     print("âš ï¸  rouge_score æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•å¥–åŠ±å‡½æ•°")

# -------------------------
# 5) GRPO config
# -------------------------
training_args = GRPOConfig(
    output_dir="./grpo_tldr_rlvr",
    learning_rate=1e-5,
    
    per_device_train_batch_size=4,      # æ¯ä¸ªè®¾å¤‡çš„batch size
    gradient_accumulation_steps=4,       # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    num_train_epochs=1,                  # è®­ç»ƒè½®æ•°ï¼ˆGRPO ä½¿ç”¨ epochs è€Œä¸æ˜¯ num_ppo_epochsï¼‰
    max_steps=200,                       # æˆ–è€…ä½¿ç”¨ max_steps æ¥æ§åˆ¶è®­ç»ƒæ­¥æ•°
    
    # ç”Ÿæˆå‚æ•°
    max_completion_length=128,            # æœ€å¤§ç”Ÿæˆ completion é•¿åº¦ï¼ˆGRPO ä½¿ç”¨ max_completion_length è€Œä¸æ˜¯ max_new_tokensï¼‰
    temperature=1.0,                     # é‡‡æ ·æ¸©åº¦
    
    # GRPO ç‰¹å®šå‚æ•°
    num_generations=4,                    # æ¯ä¸ª prompt ç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ˆç”¨äº group relative rewardsï¼‰
    scale_rewards="group",                # å¥–åŠ±æ ‡å‡†åŒ–æ–¹å¼ï¼š"group"ï¼ˆç»„å†…æ ‡å‡†åŒ–ï¼‰æˆ– "batch"ï¼ˆæ‰¹æ¬¡æ ‡å‡†åŒ–ï¼‰
    beta=0.0,                             # KL ç³»æ•°ï¼Œ0.0 è¡¨ç¤ºä¸ä½¿ç”¨ reference modelï¼ˆèŠ‚çœå†…å­˜ï¼‰
    
    # å…¶ä»–å‚æ•°
    report_to="wandb",  # æˆ– "none" æ¥ç¦ç”¨ wandb
    eval_strategy="no",                  # ä¸ä½¿ç”¨è¯„ä¼°
    logging_steps=10,                     # æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡
)

# -------------------------
# 6) GRPO Trainer (ä¸ä¾èµ– reward model)
# -------------------------
trainer = GRPOTrainer(
    model=policy_model,                  # ä½¿ç”¨å·²ç»åŠ è½½çš„æ¨¡å‹ï¼ˆå¸¦ LoRAï¼‰
    reward_funcs=tldr_reward_func,       # ä½¿ç”¨è‡ªå®šä¹‰ reward function
    args=training_args,
    processing_class=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=None,  # å·²ç»åŠ è½½äº†PeftModelï¼Œä¸éœ€è¦peft_config
)

# -------------------------
# 7) å¼€å§‹è®­ç»ƒ
# -------------------------
print("ğŸš€ å¼€å§‹ GRPO è®­ç»ƒï¼ˆRLVR æ–¹æ³•ï¼Œä¸ä¾èµ– reward modelï¼‰...")
trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model(training_args.output_dir)
print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {training_args.output_dir}")

print("âœ… è®­ç»ƒå®Œæˆï¼")
