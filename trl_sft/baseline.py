import os
import torch
from accelerate import PartialState
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    BitsAndBytesConfig,
)
from peft import PeftModel
from trl.experimental.ppo import PPOConfig, PPOTrainer

# ä¿®å¤ wandb JSON decode é”™è¯¯
# æ–¹æ¡ˆ 1: ä½¿ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆé¿å…ç½‘ç»œé—®é¢˜ï¼‰
os.environ.setdefault("WANDB_MODE", "offline")

# æ–¹æ¡ˆ 2: å¦‚æœéœ€è¦åœ¨çº¿æ¨¡å¼ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶è®¾ç½® API key
# os.environ["WANDB_API_KEY"] = "your-api-key"
# os.environ["WANDB_PROJECT"] = "your-project-name"

# æ–¹æ¡ˆ 3: å¦‚æœä»ç„¶æœ‰é—®é¢˜ï¼Œå¯ä»¥åœ¨è®­ç»ƒé…ç½®ä¸­è®¾ç½® report_to="none" æ¥ç¦ç”¨ wandb

# æ ¹æ®trl_sftåŠ å…¥äº†RLHFä¹‹åçš„æ¨¡å‹
# -------------------------
# 0) paths
# -------------------------
base_model_name = "Qwen/Qwen2.5-0.5B-Instruct"

# SFTè®­ç»ƒåçš„adapterä¿å­˜åˆ°è¿™ä¸ªç›®å½•,ä»è¿™é‡ŒåŠ è½½LoRAå¾®è°ƒçš„SFT
sft_adapter_dir = "./sft_tldr_lora/checkpoint-63"

# Reward model è·¯å¾„
# é€‰é¡¹ 1: ä½¿ç”¨è‡ªå·±è®­ç»ƒçš„ reward modelï¼ˆæ¨èï¼Œä¸ Qwen tokenizer å…¼å®¹ï¼‰
reward_model_path = "./reward_model/Qwen2.5-0.5B-Instruct-Reward"

# é€‰é¡¹ 2: ä½¿ç”¨å…¬å¼€çš„ reward modelï¼ˆæ³¨æ„ï¼štokenizer å¯èƒ½ä¸å…¼å®¹ï¼‰
# reward_model_path = "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr"

# å¦‚æœè®­ç»ƒå¥½çš„ reward model ä¸å­˜åœ¨ï¼Œå¯ä»¥ä½¿ç”¨å…¬å¼€æ¨¡å‹ä½œä¸ºå¤‡é€‰
import os
if not os.path.exists(reward_model_path) or not os.listdir(reward_model_path):
    print(f"âš ï¸  è­¦å‘Š: {reward_model_path} ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°†ä½¿ç”¨å…¬å¼€æ¨¡å‹")
    reward_model_path = "cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr"

# -------------------------
# 1) tokenizer
# -------------------------
tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side="left")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

bnb = BitsAndBytesConfig(load_in_8bit=True)

# -------------------------
# 2) policy (trainable): Qwen base + your LoRA adapter
# -------------------------
base_policy = AutoModelForCausalLM.from_pretrained(
    base_model_name, device_map="auto", quantization_config=bnb, trust_remote_code=True
)
policy_model = PeftModel.from_pretrained(base_policy, sft_adapter_dir, is_trainable=True)

# -------------------------
# 3) ref (frozen): same init as policy, but frozen
# -------------------------
base_ref = AutoModelForCausalLM.from_pretrained(
    base_model_name, device_map="auto", quantization_config=bnb, trust_remote_code=True
)
ref_model = PeftModel.from_pretrained(base_ref, sft_adapter_dir, is_trainable=False)
ref_model.eval()
for p in ref_model.parameters():
    p.requires_grad = False

# -------------------------
# 4) reward model and value model (frozen)
# -------------------------
# ä½¿ç”¨è‡ªå·±è®­ç»ƒçš„ reward modelï¼ˆåŸºäº Qwenï¼Œtokenizer å…¼å®¹ï¼‰
print(f"ğŸ“¦ åŠ è½½ reward model from: {reward_model_path}")

# å…ˆåŠ è½½ config æ¥æ£€æŸ¥ num_labels
from transformers import AutoConfig
try:
    reward_config = AutoConfig.from_pretrained(reward_model_path, trust_remote_code=True)
    # å¦‚æœ config ä¸­æœ‰ num_labelsï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™é»˜è®¤ä½¿ç”¨ 1
    num_labels = getattr(reward_config, 'num_labels', 1)
    print(f"   æ£€æµ‹åˆ° num_labels: {num_labels}")
except Exception as e:
    print(f"   âš ï¸  æ— æ³•è¯»å– configï¼Œä½¿ç”¨é»˜è®¤ num_labels=1: {e}")
    num_labels = 1

# åŠ è½½ reward model
# æ³¨æ„ï¼šå¦‚æœ checkpoint ä¸­çš„ num_labels ä¸æŒ‡å®šå€¼ä¸åŒ¹é…ï¼Œä¼šæŠ¥é”™
# è§£å†³æ–¹æ¡ˆï¼šä¸æŒ‡å®š num_labelsï¼Œè®©æ¨¡å‹ä» checkpoint ä¸­è‡ªåŠ¨è¯»å–
reward_model = AutoModelForSequenceClassification.from_pretrained(
    reward_model_path, 
    # ä¸æŒ‡å®š num_labelsï¼Œè®©æ¨¡å‹ä» checkpoint çš„ config ä¸­è¯»å–
    device_map="auto", 
    trust_remote_code=True,
)
reward_model.eval()
for p in reward_model.parameters():
    p.requires_grad = False

# value_model é€šå¸¸å’Œ reward_model ç›¸åŒ
value_model = AutoModelForSequenceClassification.from_pretrained(
    reward_model_path, 
    # ä¸æŒ‡å®š num_labelsï¼Œè®©æ¨¡å‹ä» checkpoint ä¸­è‡ªåŠ¨è¯»å–
    device_map="auto", 
    trust_remote_code=True,
)
value_model.eval()
for p in value_model.parameters():
    p.requires_grad = False

# å¦‚æœ num_labels=2ï¼Œéœ€è¦åˆ›å»ºåŒ…è£…å™¨æ¥é€‚é… PPO trainer çš„æœŸæœ›
# PPO trainer æœŸæœ› reward/value model è¾“å‡ºå½¢çŠ¶ä¸º [batch, seq] æˆ– [batch, seq, 1]
if num_labels == 2:
    print("   âš ï¸  æ£€æµ‹åˆ° num_labels=2ï¼Œåˆ›å»ºåŒ…è£…å™¨ä»¥é€‚é… PPO trainer")
    
    class RewardModelWrapper(torch.nn.Module):
        """åŒ…è£…å™¨ï¼šå°† num_labels=2 çš„è¾“å‡ºè½¬æ¢ä¸º num_labels=1 çš„è¾“å‡º"""
        def __init__(self, model):
            super().__init__()
            self.model = model
            self.base_model_prefix = model.base_model_prefix
            
        def score(self, hidden_states):
            """åªå–ç¬¬ä¸€ä¸ªç»´åº¦çš„è¾“å‡ºï¼ˆæˆ–è€…å¯ä»¥å–å¹³å‡ï¼Œæ ¹æ®è®­ç»ƒæ–¹å¼ï¼‰"""
            scores = self.model.score(hidden_states)  # [batch, seq, 2]
            # å–ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œæˆ–è€…æ ¹æ®è®­ç»ƒæ–¹å¼è°ƒæ•´
            # å¦‚æœæ˜¯ preference learningï¼Œå¯èƒ½éœ€è¦å– chosen - rejected
            # è¿™é‡Œå‡è®¾ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯ reward score
            return scores[..., 0:1]  # ä¿æŒç»´åº¦ [batch, seq, 1]ï¼Œè¿™æ · squeeze(-1) åæ˜¯ [batch, seq]
        
        def __getattr__(self, name):
            """è½¬å‘å…¶ä»–å±æ€§åˆ°åŸå§‹æ¨¡å‹"""
            try:
                return super().__getattr__(name)
            except AttributeError:
                return getattr(self.model, name)
    
    # åŒ…è£… reward model å’Œ value model
    reward_model = RewardModelWrapper(reward_model)
    value_model = RewardModelWrapper(value_model)
    print("   âœ… Reward model å’Œ Value model åŒ…è£…å®Œæˆ")

print("âœ… Reward model å’Œ value model åŠ è½½å®Œæˆ")

# -------------------------
# 5) TL;DR dataset -> PPO queries
# -------------------------
raw_dataset = load_dataset("trl-lib/tldr", split="train[:1000]")

def prepare_dataset(dataset, tokenizer):
    """pre-tokenize the dataset before training; only collate during training"""

    def tokenize(element):
        input_ids = tokenizer(element["prompt"], padding=False)["input_ids"]
        return {"input_ids": input_ids, "lengths": len(input_ids)}

    return dataset.map(
        tokenize,
        remove_columns=dataset.column_names,
    )

# Compute that only on the main process for faster data processing.
with PartialState().local_main_process_first():
    train_dataset = prepare_dataset(raw_dataset, tokenizer)
    # filtering
    train_dataset = train_dataset.filter(lambda x: x["lengths"] <= 512)

assert train_dataset[0]["input_ids"][-1] != tokenizer.eos_token_id, "The last token should not be an EOS token"

# åˆ›å»ºä¸€ä¸ªå°çš„ eval_datasetï¼ˆç”¨äº generate_completionsï¼Œå³ä½¿ eval_strategy="no"ï¼‰
# PPO trainer éœ€è¦ eval_dataset æ¥ç”Ÿæˆç¤ºä¾‹ï¼Œå³ä½¿ä¸ä½¿ç”¨è¯„ä¼°
eval_dataset = train_dataset.select(range(min(10, len(train_dataset))))

# -------------------------
# 6) PPO config
# -------------------------
training_args = PPOConfig(
    output_dir="./ppo_tldr_rlhf_baseline",
    learning_rate=1e-5,
    
    per_device_train_batch_size=4,      # æ¯ä¸ªè®¾å¤‡çš„batch size
    gradient_accumulation_steps=4,       # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    local_rollout_forward_batch_size=16, # rollouté˜¶æ®µçš„forward batch size
    num_ppo_epochs=4,                    # PPO epochs
    
    # ç”Ÿæˆå‚æ•°
    response_length=128,                # å“åº”é•¿åº¦
    temperature=1.0,                     # é‡‡æ ·æ¸©åº¦
    stop_token="eos",                    # åœæ­¢token
    
    kl_coef=0.1,                        # KLæ•£åº¦ç³»æ•°
    
    # å…¶ä»–å‚æ•°
    # å¦‚æœ wandb ä»æœ‰é—®é¢˜ï¼Œå¯ä»¥æ”¹ä¸º "none" æˆ– [] æ¥ç¦ç”¨
    report_to="wandb",  # æˆ– "none" æ¥ç¦ç”¨ wandb
    eval_strategy="no",                  # ä¸ä½¿ç”¨è¯„ä¼°
    num_sample_generations=0,            # ç¦ç”¨ generate_completionsï¼ˆéœ€è¦ eval_datasetï¼‰
)

# -------------------------
# 7) PPO Trainer
# -------------------------
trainer = PPOTrainer(
    args=training_args,
    processing_class=tokenizer,
    model=policy_model,
    ref_model=ref_model,
    reward_model=reward_model,
    value_model=value_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,  # æä¾› eval_dataset ç”¨äº generate_completions
    peft_config=None,  # å·²ç»åŠ è½½äº†PeftModelï¼Œä¸éœ€è¦peft_config
)

# -------------------------
# 8) å¼€å§‹è®­ç»ƒ
# -------------------------
trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model(training_args.output_dir)

print("Done.")
